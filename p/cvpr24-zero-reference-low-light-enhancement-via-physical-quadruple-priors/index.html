<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='QuadPrior Homepage https://daooshee.github.io/QuadPrior-Website/ arXiv https://arxiv.org/abs/2403.12933 作者：Wenjing Wang, Huan Yang, Jianlong Fu, Jiaying Liu 摘要 在弱光环境下，理解光照和减少监督需求是一项重大挑战。目前的方法对训练过程中的数据使用和特定光'><title>CVPR24 Zero-Reference Low-Light Enhancement via Physical Quadruple Priors</title>
<link rel=canonical href=https://jrhim.com/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/><link rel=stylesheet href=/scss/style.min.36658f78ed70a72fbfd31cce296a92ab2d2153c13fdd0b56494e8fd1cad904df.css><meta property='og:title' content='CVPR24 Zero-Reference Low-Light Enhancement via Physical Quadruple Priors'><meta property='og:description' content='QuadPrior Homepage https://daooshee.github.io/QuadPrior-Website/ arXiv https://arxiv.org/abs/2403.12933 作者：Wenjing Wang, Huan Yang, Jianlong Fu, Jiaying Liu 摘要 在弱光环境下，理解光照和减少监督需求是一项重大挑战。目前的方法对训练过程中的数据使用和特定光'><meta property='og:url' content='https://jrhim.com/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/'><meta property='og:site_name' content='JrHimself'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='CVPR'><meta property='article:tag' content='LLE'><meta property='article:tag' content='Physics-Guided'><meta property='article:published_time' content='2024-03-25T00:00:00+00:00'><meta property='article:modified_time' content='2024-03-25T00:00:00+00:00'><meta property='og:image' content='https://jrhim.com/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/cover.png'><meta name=twitter:title content="CVPR24 Zero-Reference Low-Light Enhancement via Physical Quadruple Priors"><meta name=twitter:description content="QuadPrior Homepage https://daooshee.github.io/QuadPrior-Website/ arXiv https://arxiv.org/abs/2403.12933 作者：Wenjing Wang, Huan Yang, Jianlong Fu, Jiaying Liu 摘要 在弱光环境下，理解光照和减少监督需求是一项重大挑战。目前的方法对训练过程中的数据使用和特定光"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://jrhim.com/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/cover.png'><link rel="shortcut icon" href=/favicon.png><script type=text/javascript src=https://demo.hellozwh.com/source/canvas-nest.min.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=https://avatars.githubusercontent.com/u/64763790 width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🤗</span></figure><div class=site-meta><h1 class=site-name><a href=/>JrHimself</a></h1><h2 class=site-description>跳动的世界里找你的频率</h2></div></header><ol class=social-menu><li><a href=https://github.com/hongjr03 target=_blank title=@hongjr03 rel=me><svg width="800" height="800" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"/><path d="M12 2C6.475 2 2 6.475 2 12a9.994 9.994.0 006.838 9.488c.5.087.687-.213.687-.476.0-.237-.013-1.024-.013-1.862-2.512.463-3.162-.612-3.362-1.175-.113-.288-.6-1.175-1.025-1.413-.35-.187-.85-.65-.013-.662.788-.013 1.35.725 1.538 1.025.9 1.512 2.338 1.087 2.912.825.088-.65.35-1.087.638-1.337-2.225-.25-4.55-1.113-4.55-4.938.0-1.088.387-1.987 1.025-2.688-.1-.25-.45-1.275.1-2.65.0.0.837-.262 2.75 1.026a9.28 9.28.0 012.5-.338c.85.0 1.7.112 2.5.337 1.912-1.3 2.75-1.024 2.75-1.024.55 1.375.2 2.4.1 2.65.637.7 1.025 1.587 1.025 2.687.0 3.838-2.337 4.688-4.562 4.938.362.312.675.912.675 1.85.0 1.337-.013 2.412-.013 2.75.0.262.188.574.688.474A10.016 10.016.0 0022 12c0-5.525-4.475-10-10-10z"/></g></svg></a></li><li><a href=https://space.bilibili.com/18763596 target=_blank title=西西里阿西弗斯 rel=me><svg width="800" height="800" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"/><path d="M18.223 3.086a1.25 1.25.0 010 1.768L17.08 5.996h1.17A3.75 3.75.0 0122 9.747v7.5a3.75 3.75.0 01-3.75 3.75H5.75A3.75 3.75.0 012 17.247v-7.5a3.75 3.75.0 013.75-3.75h1.166L5.775 4.855a1.25 1.25.0 111.767-1.768l2.652 2.652c.079.079.145.165.198.257h3.213c.053-.092.12-.18.199-.258l2.651-2.652a1.25 1.25.0 011.768.0zm.027 5.42H5.75A1.25 1.25.0 004.503 9.663l-.003.094v7.5c0 .659.51 1.199 1.157 1.246l.093.004h12.5a1.25 1.25.0 001.247-1.157l.003-.093v-7.5c0-.69-.56-1.25-1.25-1.25zm-10 2.5c.69.0 1.25.56 1.25 1.25v1.25a1.25 1.25.0 11-2.5.0v-1.25c0-.69.56-1.25 1.25-1.25zm7.5.0c.69.0 1.25.56 1.25 1.25v1.25a1.25 1.25.0 11-2.5.0v-1.25c0-.69.56-1.25 1.25-1.25z"/></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>首页</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于我</span></a></li><li><a href=https://raw.githubusercontent.com/hongjr03/hongjr03.github.io/master/content/page/files/cv/cv_Jiarong.pdf target=_blank><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-file-cv"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3v4a1 1 0 001 1h4"/><path d="M17 21H7a2 2 0 01-2-2V5a2 2 0 012-2h7l5 5v11a2 2 0 01-2 2z"/><path d="M11 12.5a1.5 1.5.0 00-3 0v3a1.5 1.5.0 003 0"/><path d="M13 11l1.5 6 1.5-6"/></svg>
<span>简历</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://jrhim.com/ selected></option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#摘要>摘要</a></li><li><a href=#贡献点>贡献点</a></li><li><a href=#基于物理先验的图像复原>基于物理先验的图像复原</a><ol><li><a href=#可学习的照明不变先验>可学习的照明不变先验</a><ol><li><a href=#物理四重先验>物理四重先验</a></li><li><a href=#通过神经网络学习>通过神经网络学习</a></li><li><a href=#物理解释>物理解释</a></li></ol></li><li><a href=#通过扩散模型构建先验到图像的框架>通过扩散模型构建先验到图像的框架</a><ol><li><a href=#去噪>去噪</a></li><li><a href=#蒸馏提高效率>蒸馏提高效率</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/><img src=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/cover_hua0181e830421e351cff51a407716e9c7_3632983_800x0_resize_box_3.png srcset="/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/cover_hua0181e830421e351cff51a407716e9c7_3632983_800x0_resize_box_3.png 800w, /p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/cover_hua0181e830421e351cff51a407716e9c7_3632983_1600x0_resize_box_3.png 1600w" width=800 height=533 loading=lazy alt="Featured image of post CVPR24 Zero-Reference Low-Light Enhancement via Physical Quadruple Priors"></a></div><div class=article-details><header class=article-category><a href=/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/>计算机视觉</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/>CVPR24 Zero-Reference Low-Light Enhancement via Physical Quadruple Priors</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 25, 2024</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 8 分钟</time></div></footer></div></header><section class=article-content><div class=article-list--compact><article><a href=https://daooshee.github.io/QuadPrior-Website/><div class=article-details><div class=article-title>QuadPrior Homepage</div><div class=article-preview>https://daooshee.github.io/QuadPrior-Website/</div></div></a></article></div><p></p><div class=article-list--compact><article><a href=https://arxiv.org/abs/2403.12933><div class=article-details><div class=article-title>arXiv</div><div class=article-preview>https://arxiv.org/abs/2403.12933</div></div></a></article></div><p></p><p>作者：<a class=link href=https://daooshee.github.io/website/ target=_blank rel=noopener>Wenjing Wang</a>, <a class=link href=https://hyang0511.github.io/ target=_blank rel=noopener>Huan Yang</a>, <a class=link href=https://www.microsoft.com/en-us/research/people/jianf/ target=_blank rel=noopener>Jianlong Fu</a>, <a class=link href=http://www.wict.pku.edu.cn/struct/people/liujiaying.html target=_blank rel=noopener>Jiaying Liu</a></p><p><img src=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Comp1.png width=4267 height=1714 srcset="/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Comp1_hu390c9e17af459cb9dc8cc10e037022b4_7632152_480x0_resize_box_3.png 480w, /p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Comp1_hu390c9e17af459cb9dc8cc10e037022b4_7632152_1024x0_resize_box_3.png 1024w" loading=lazy alt=Comp class=gallery-image data-flex-grow=248 data-flex-basis=597px></p><h2 id=摘要>摘要</h2><p>在弱光环境下，理解光照和减少监督需求是一项重大挑战。目前的方法对训练过程中的数据使用和特定光照的超参数高度敏感，限制了它们处理未知场景的能力。在本文中，我们提出了一种新的零参考低照度增强框架，该框架可仅使用正常光照下的图像进行训练。为此，我们从物理光传递理论中汲取灵感，设计了一种光照不变先验。这个先验值是连接正常光线图像和弱光图像的桥梁。然后，我们开发了一个先验图像框架，在没有弱光数据的情况下进行训练。在测试过程中，该框架能够将我们的光照不变先验恢复到图像中，自动实现弱光增强。在这一框架内，我们利用预训练生成扩散模型来提高模型能力，引入旁路解码器来处理细节失真，并提供一个轻量级版本以提高实用性。广泛的实验证明了我们的框架在各种情况下的优越性，以及良好的可解释性、鲁棒性和效率。</p><h2 id=贡献点>贡献点</h2><p>开发了一个光照不变先验，源于 Kubelka-Munk 理论，将其作为低照度和正常光照图像之间的桥梁。训练时，仅使用正常光照图像，模型从图像分布中学习明亮照明的概念；在测试时，模型会自动提取低照度图像与光照无关的特征，然后转移到正常光照的图像中。这样，无需任何低照度图像、或者与照度有关的超参数，模型就能够在未知场景中进行低照度增强。</p><p>总结：</p><ul><li>我们提出了一种零参考低照度增强模型，该模型利用照度不变先验作为不同照度之间的中介。我们的模型无需依赖任何特定的低照度数据，就能在各种低照度场景中表现出卓越的性能。</li><li>我们建立了物理四重先验，这是一种新颖的可学习的光照不变先验，源自光传递理论。该先验捕捉到了不同光照条件下成像的本质，使弱光增强摆脱了对参考样本或人为设置的超参数的依赖。</li><li>我们开发了一种有效的先验 - 图像映射系统，将先验作为控制预训练大规模生成扩散模型的条件。我们引入了一个旁路解码器来解决失真问题，并证明我们的模型可以提炼成一个轻量级版本以用于实际应用。</li></ul><p><img src=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Overall.png width=3277 height=2471 srcset="/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Overall_hu27511af975c69d138a42b76b037f7c6d_1734430_480x0_resize_box_3.png 480w, /p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Overall_hu27511af975c69d138a42b76b037f7c6d_1734430_1024x0_resize_box_3.png 1024w" loading=lazy alt=QUAD_PRIOR_CVPR24 class=gallery-image data-flex-grow=132 data-flex-basis=318px></p><h2 id=基于物理先验的图像复原>基于物理先验的图像复原</h2><h3 id=可学习的照明不变先验>可学习的照明不变先验</h3><h4 id=物理四重先验>物理四重先验</h4><p>从光传递的 Kubelka-Munk 理论出发。给定波长 $\lambda$，图像平面上位于空间位置 $\mathbf{x}$ 的入射光谱能量模型为</p><p>$$
E(\lambda,\mathbf{x}) = e(\lambda, \mathbf{x})\left((1-i(\mathbf{x}))^2R_\infty(\lambda,\mathbf{x})+i(\mathbf{x})\right)\text{.}\tag{1}
$$</p><p>其中，$e(\lambda, \mathbf{x})$ 表示光源光谱，$i(\mathbf{x})$ 表示镜面反射，$R_\infty(\lambda,\mathbf{x})$ 表示材料反射率。当物体是无光泽的时候，即 $i(\mathbf{x}) \approx 0$，式 (1) 可以简化为</p><p>$$
E(\lambda,\mathbf{x}) = e(\lambda, \mathbf{x})R_\infty(\lambda,\mathbf{x})\text{,}\tag{2}
$$</p><p>和 Retinex 模型相同。也就是说，Retinex 理论可以看作是 Kubelka-Munk 理论的一个特例。</p><p>首先，为了简便起见，我们将一些变量表示为</p><p>$$
E^{\lambda} = \frac {\partial E(\lambda,\mathbf{x})} {\partial \lambda}, ;
R_\infty^{\lambda} = \frac {\partial R_\infty(\lambda,\mathbf{x})} {\partial \lambda}\text{,} \tag{3}
$$
$$
E^{\lambda\lambda} = \frac {\partial^2 E(\lambda,\mathbf{x})} {\partial \lambda^2}, ;
R_\infty^{\lambda\lambda} = \frac {\partial^2 R_\infty(\lambda,\mathbf{x})} {\partial \lambda^2}\text{.} \tag{4}
$$</p><p>直观地，$E$ 表示光谱强度，$E^\lambda$ 表示光谱斜率，$E^{\lambda\lambda}$ 表示光谱曲率。</p><p>根据 <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，通过简化假设，我们可以从式 (1) 得到一系列不变量。主要思路是，去掉 $i$ 和 $e$ 的影响，只保留 $R_\infty$ 的影响。由于 $R_\infty$ 是材料的固有属性，因此在不同光照条件下，$R_\infty$ 是不变的。因此导出的变量可以表现出光照不变性。</p><ul><li><p>假设等能量照明，即 $e(\lambda, \mathbf{x})$ 被简化为与 $\lambda$ 无关的 $e(\mathbf{x})$，那么式 (1) 可以简化为</p><p>$$
E(\lambda,\mathbf{x}) = \tilde{e}(\mathbf{x})\left((1-i(\mathbf{x}))^2R_\infty(\lambda,\mathbf{x})+i(\mathbf{x})\right)\text{,}\tag{5}
$$</p><p>将式 (5) 代入 $E^{\lambda}/E^{\lambda\lambda}$ 可得</p><p>$$
\frac{E^{\lambda}} {E^{\lambda\lambda}} = \frac {\tilde{e}(\mathbf{x})(1-i(\mathbf{x}))^2R_\infty^{\lambda}} {\tilde{e}(\mathbf{x})(1-i(\mathbf{x}))^2R_\infty^{\lambda\lambda}} = \frac{R_\infty^{\lambda}} {R_\infty^{\lambda\lambda}}\text{,}\tag{6}
$$</p><p>其中照明属性 $i$ 和 $e$ 被消除，只剩下材料属性 $R_\infty$。由于它和光照无关，因此它建立了 $E^{\lambda}/E^{\lambda\lambda}$ 的光照不变性。这样，第一个光照不变量是</p><p>$$
H = \arctan \left( {E^{\lambda}} / {E^{\lambda\lambda}} \right) \text{.}\tag{7}
$$</p></li><li><p>再假设表面是无光泽的，即 $i(\mathbf{x}) \approx 0$，那么式 (1) 可以简化为</p><p>$$
E(\lambda,\mathbf{x}) = \tilde{e}(\mathbf{x})R_\infty(\lambda,\mathbf{x})\text{,}\tag{8}
$$</p><p>类似地，我们可以推导出第二个光照不变量</p><p>$$
C = \log \left( \frac { (E^{\lambda})^2 + (E^{\lambda\lambda})^2} {E(\lambda,\mathbf{x})^2} \right) \nonumber \
= \log \left( \frac { (R_\infty^{\lambda})^2 + (R_\infty^{\lambda\lambda})^2} {R_\infty(\lambda,\mathbf{x})^2} \right) \text{.}\tag{9}
$$</p></li><li><p>进一步假设光照均匀，即 $\tilde{e}(\mathbf{x})$ 简化为 $\bar{e}$，那么式 (1) 可以简化为</p><p>$$
E(\lambda,\mathbf{x}) = \bar{e}R_\infty(\lambda,\mathbf{x})\text{,}\tag{10}
$$</p><p>类似地，我们可以推导出第三个光照不变量</p><p>$$
W = \tan \left( \bigg| \frac {\partial E(\lambda,\mathbf{x})} {\partial \mathbf{x}} \frac 1 {E(\lambda,\mathbf{x})} \bigg| \right) \nonumber \
= \tan \left( \bigg| \frac {\partial R_\infty(\lambda,\mathbf{x})} {\partial \mathbf{x}} \frac 1 {R_\infty(\lambda,\mathbf{x})} \bigg| \right) \text{.}\tag{11}
$$</p></li></ul><p>Kubelka-Munk 理论对灰度图像很有效，但在色彩的解释上有一定的局限性。上述的三个光照不变式丢失了部分色彩信息。因此对于第四个光照不变式，我们描述了 RGB 三个通道像素值之间的相对关系。</p><ul><li>假设光照会保持颜色的顺序，我们将 RGB 三个通道的顺序作为基本的光照不变特征，记为 $O$。</li></ul><h4 id=通过神经网络学习>通过神经网络学习</h4><p>我们通过高斯色彩模型<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>和 CIConv<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> 来从 RGB 图像中获取先验。首先，我们通过线性映射估计观测到的能量 $\hat{E}$ 和它的导数 $\hat{E}^\lambda$、$\hat{E}^{\lambda\lambda}$</p><p>$$
\begin{bmatrix}
\hat{E}(x,y) \newline \hat{E}^\lambda(x,y) \newline \hat{E}^{\lambda\lambda}(x,y) \newline \end{bmatrix} = \mathcal{W}
\begin{bmatrix} R(x,y) \newline G(x,y) \newline B(x,y) \end{bmatrix}
\text{,}\tag{12}
$$</p><p>其中，$x$ 和 $y$ 分别表示图像中的位置，$\mathcal{W}$ 是 $3\times 3$ 的矩阵。在 <sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref1:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> 中，$\mathcal{W}$ 是手动设计的，而我们用我们的先验到图像框架通过自然图像的分布来学习它。</p><p>公式 (11) 中的空间导数 $\partial E / \partial \mathbf{x}$ 是在 x 和 y 方向上计算的，表示为 $\partial E / \partial \mathbf{x}=(E_x, E_y)$ ，其大小由 $|\partial E / \partial \mathbf{x}| = \sqrt{E_x^2 + E_y^2}$ 给出。最终，通过将 $\hat{E}$ 与高斯颜色平滑和尺度为 $\sigma$ 的导数滤波器进行卷积来估计 $E$、$E_x$ 和 $E_y$ 。$\sigma$ 是从输入图像中预测得出的。类似地，$E^{\lambda}$ 是从 $\hat{E}^{\lambda}$ 获得的，$E^{\lambda\lambda}$ 是从 $\hat{E}^{\lambda\lambda}$ 获得的。现在我们可以从输入图像中计算$H$、$C$ 和 $W$。</p><p>我们的最后一个光照不变量，即 RGB 通道的顺序，定义为如下三个通道</p><p>$$
O(x,y) = \left[ O_R(x,y), O_G(x,y), O_B(x,y) \right]\text{,}\tag{13}
$$</p><p>其中，$O_R$ 表示 RGB 图像中 $R$ 通道的顺序，标准化为 $[-1,1]$。$O_G$ 和 $O_B$ 同理。</p><p>最后，$H$、$C$、$W$ 和 $O$ 在通道维度上连接在一起，形成我们的物理四元先验。</p><h4 id=物理解释>物理解释</h4><p>首先，数学形式表示 $W$ 代表光谱强度的强度归一化空间导数。至于 $H$，根据<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，它和材料的色调有关，即材料的 $\arctan(\lambda_\text{max})$。而对于 $C$，在基于光谱波长的色环内，色调代表角度，而色度则是距离中心的距离。此外，当将笛卡尔坐标 $(a, b)$ 转换为极坐标时，角度变为 $\arctan(b/a)$ ，半径变为 $\sqrt{(a)^2+(b)^2}$ 。将这与方程 (7) 和方程 (9) 连接起来，我们发现 C 与色度相关联。</p><h3 id=通过扩散模型构建先验到图像的框架>通过扩散模型构建先验到图像的框架</h3><p>理想情况下，我们希望保留所有与光照无关的信息，同时舍弃与光照相关的信息。然而，实现这种分解具有挑战性，仍然是图像建模中尚未解决的问题。尽管我们的物理四元先验（即 $H$、$C$、$W$ 和 $O$）从不同角度捕捉了与光照无关的信息，但仍会丢失一些信息。因此，根据先验重建图像并不容易。</p><p>我们建议使用大模型来直接补全缺失的信息，而不是专注于改进光照不变先验。我们使用 Stable Diffusion v1-5 并使用 ControlNet 框架将其转为条件生成模式。将四个物理先验作为条件来控制生成模型。</p><p><img src=https://github.com/daooshee/QuadPrior/raw/main/Framework.jpg loading=lazy alt=QUAD_PRIOR_CVPR24></p><p>在训练过程中，使用一个冻结的 SD 编码器将图像 $I$ 映射到压缩的潜在表示 $z_0$ 中。然后，我们在随机时间步长 $t\in{1,&mldr;,T}$ 处对 $z_t$ 进行采样，通过</p><p>$$
z_{t}=\sqrt{\bar{\alpha}_t}z_0+\sqrt{1-\bar{\alpha}_t} \epsilon\text{,}\tag{14}
$$</p><p>其中 ${\bar{\alpha}_t}$ 是一个预定义参数序列。训练目标是根据 $z_t$ 和我们的先验预测 $\epsilon$。起初，SD 利用 U-Net 来从 $z_t$ 预测 $\epsilon$，但 U-Net 现在被我们冻结了。我们添加了一组编码模块，以从我们的四重先验中提取特征。然后将这些特征纳入 SD U-Net 中。我们采用了零卷积策略<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>，以确保在训练开始时，新的层不会影响原始 SD。在测试过程中，给定输入图像 $I$，我们提取物理四元先验，并将其作为反向扩散过程中预测 $z_0$ 的条件。随后，通过解码器将 $z_0$ 投影回图像空间。</p><p>虽然 ControlNet 在各种应用中都取得了成功，但直接应用它却存在收敛速度慢、细节退化和依赖文本提示等问题。为了解决这些问题，使其更适合我们的图像修复任务，我们进行了以下改进。</p><ul><li><p>在典型的扩散模型中，训练目标是预测高斯噪声项：</p><p>$$
\mathcal{L}_{\text{noise}} = ||\epsilon-\hat{\epsilon}||^2_2\text{.}\tag{15}
$$</p><p>我们额外将 $z_0$ 的差值最小化，这可以加快收敛速度。结合公式 (14)，可以得出</p><p>$$
\mathcal{L}_{z_0} = ||z_0-\hat{z}_0||^2_2 =
||z_0 - \frac{z_t-\sqrt{1-\bar{\alpha}_t}\hat{\epsilon}} {\sqrt{\bar{\alpha}_t}} ||^2_2\text{.}\tag{16}
$$</p><p>我们简单地将这两种损失合并为最终损失</p><div>$$
\mathcal{L}_{\text{DIFF}} = \mathcal{L}_{z_0} + \mathcal{L}_{\text{noise}}\text{.}\tag{17}
$$</div></li><li><p>SD 采用自动编码器将图像 $I$ 压缩成潜在表示 $z_0$，从而降低了计算成本。但是，自动编码器会带来严重的细节失真。为了缓解这一问题，我们利用编码器的特征为解码器提供支持，并设计了一种有效的微调策略。</p><p><img src=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/AE.png width=2545 height=971 srcset="/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/AE_huf47b2858d96a0a74c5310bf9429d84c9_627378_480x0_resize_box_3.png 480w, /p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/AE_huf47b2858d96a0a74c5310bf9429d84c9_627378_1024x0_resize_box_3.png 1024w" loading=lazy alt="The training strategy of our bypass decoder." class=gallery-image data-flex-grow=262 data-flex-basis=629px></p><p>如上图所示，在训练过程中，我们用随机光照抖动和噪声扭曲输入图像 $I$，得到 $\tilde{I}$。然后解码器结合从 $\tilde{I}$ 中提取的特征 $z^1$、$z^2$、$z^3$ 还原 $z_0$。我们引入了几个用于特征融合的卷积层和一个用于后处理的残差块。这些附加层初始化为零或 <code>self</code>，确保在训练开始时对原始解码过程的影响最小。新的解码器被命名为旁路解码器。</p><p><img src=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/AE-train-comparison.png width=2553 height=1097 srcset="/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/AE-train-comparison_hubfe0036cfb27cb4dbfab28af1c86e332_3151154_480x0_resize_box_3.png 480w, /p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/AE-train-comparison_hubfe0036cfb27cb4dbfab28af1c86e332_3151154_1024x0_resize_box_3.png 1024w" loading=lazy alt="Image restoration effect of the SD decoder and ours." class=gallery-image data-flex-grow=232 data-flex-basis=558px></p><p>如上图所示，我们的旁路解码器实现了明显的细节还原。如下图所示，在测试过程中，输入图像中的特征会辅助潜解码过程。我们的旁路解码器利用从输入图像中提取的 $z^1$、$z^2$ 和 $z^3$ 来重建细节，并在 $\hat{z}_0$ 中保持增强的光照度。</p><p><img src=/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Inference.png width=918 height=363 srcset="/p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Inference_hub68ec3184f89234b1d28c8e09c6abbc5_45995_480x0_resize_box_3.png 480w, /p/cvpr24-zero-reference-low-light-enhancement-via-physical-quadruple-priors/imgs/Inference_hub68ec3184f89234b1d28c8e09c6abbc5_45995_1024x0_resize_box_3.png 1024w" loading=lazy alt="The inference pipeline of our overall framework." class=gallery-image data-flex-grow=252 data-flex-basis=606px></p></li><li><p>Stable Diffusion 最开始是作为文本到图像模型设计的。然而，要求用户为低照度增强提供文本并不方便。为了解决这个问题，我们将文本输入设置为始终为空字符串。</p></li></ul><h4 id=去噪>去噪</h4><p>噪声是弱光增强中的一个重大挑战。虽然我们的先验并不是为去噪目的而设计的，但我们采用了一种简单的策略来抑制噪声。在训练过程中，我们对输入图像 $I$ 应用随机高斯 - 泊松复合噪声，同时提取物理四元先验。这种方法可以引导模型忽略高频细节，只关注低频与光照无关的信息。</p><h4 id=蒸馏提高效率>蒸馏提高效率</h4><p>扩散模型在推理中需要多步优化。即使使用 DPM-Solver++<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>，10 个步骤仍然非常繁琐。为了追求实用性，我们的框架可以创建一个更轻量级的版本。简而言之，我们构建了一个由残差块组成的轻量级 U-Net，并在瓶颈处集成了 Restormer<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> 的 Transformer 块。事实证明，Transformer 在低级视觉中非常有效。然后，我们使用我们的框架制作 1.7k 个样本来训练轻量级模型。训练目标仅为 L1 损失。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Jan-Mark Geusebroek, Rein van den Boomgaard, Arnold W. M. Smeulders, and Hugo Geerts. Color invariance. IEEE TPAMI, 23(12):1338–1350, 2001.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Theo Gevers, Arjan Gijsenij, Joost Van de Weijer, and Jan-Mark Geusebroek. Color in computer vision: Fundamentals and applications. John Wiley & Sons, 2012.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Attila Lengyel, Sourav Garg, Michael Milford, and Jan C. van Gemert. Zero-shot domain adaptation with a physics prior. In ICCV, 2021.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv, 2022.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></section><footer class=article-footer><section class=article-tags><a href=/tags/cvpr/>CVPR</a>
<a href=/tags/lle/>LLE</a>
<a href=/tags/physics-guided/>Physics-Guided</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/tip23-pugan-a-physics-guided-generative-adversarial-network-for-underwater-image-enhancement/><div class=article-image><img src=/p/tip23-pugan-a-physics-guided-generative-adversarial-network-for-underwater-image-enhancement/cover.15f3375a933c7c586f16c1adf758847a_hu134c27b5176deb4f4bd88b1d0d61a121_4891318_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post TIP23 PUGAN: A Physics-Guided Generative Adversarial Network for Underwater Image Enhancement" data-hash="md5-FfM3WpM8fFhvFsGt91iEeg=="></div><div class=article-details><h2 class=article-title>TIP23 PUGAN: A Physics-Guided Generative Adversarial Network for Underwater Image Enhancement</h2></div></a></article><article><a href=/p/cvpr04-clear-underwater-vision/><div class=article-details><h2 class=article-title>CVPR04 Clear Underwater Vision</h2></div></a></article><article><a href=/p/mm23-enhancing-visibility-in-nighttime-haze-images-using-guided-apsf-and-gradient-adaptive-convolution/><div class=article-details><h2 class=article-title>MM23 Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=hongjr03/hongjr03.github.io data-repo-id=R_kgDOK_vbTw data-category=Comments data-category-id=DIC_kwDOK_vbT84Cd52G data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><div class=post-password></div><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 JrHimself</section><section class=powerby>Blessed be the mystery of love.<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>